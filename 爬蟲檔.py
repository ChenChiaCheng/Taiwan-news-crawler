# -*- coding: utf-8 -*-
"""爬蟲檔.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BQne87or8gOF_E7sniDMpUgutX3Eo_2-
"""

from google.colab import drive
drive.mount('/content/drive')

import requests
from bs4 import BeautifulSoup
import pandas as pd
import pandas
from datetime import datetime, timedelta
import time
import datetime
from pandas import Series, DataFrame



now = datetime.datetime.now()
n = now.strftime("%Y-%m-%d")

df2 = None
result = {"時間":[],
        "標題":[],
        "連結":[],
        "內容":[],
        "來源":"自由時報"}


for page in range(1,11): 
  url = "https://news.ltn.com.tw/topic/%E6%AD%A6%E6%BC%A2%E8%82%BA%E7%82%8E/"+str(page)
  response = requests.get(url)
  soup = BeautifulSoup(response.text, 'html.parser')
  #print(soup)

  tags=soup.select(".tit")
  for i in tags:
    result["標題"].append(i.text)

  dt=soup.select(" li > span")
  for j in dt:      
    result["時間"].append(j.text)
       #print(j.text)
  
  for link in soup.select("li > .tit "): 
    links = link.get('href')
    result["連結"].append(links)
    #print(link.get('href'))
    
    r = requests.get(links)
    web_content = r.text
    soup = BeautifulSoup(web_content,'lxml')

    articleContent = soup.find_all('p')
    articleContent

    article = []
    for p in articleContent:
      article.append(p.text)

    articleAll = '\n'.join(article)
    result["內容"].append(articleAll)



'''
print(len(result["連結"]))
print(len(result["時間"]  ))
print(len(result["標題"]  ))
'''
df = pd.DataFrame(result)
print(df)
df.to_csv("./ltn_covid_content.csv")

import requests
from bs4 import BeautifulSoup
import pandas as pd
import pandas
from datetime import datetime, timedelta
import time
import datetime
from pandas import Series, DataFrame




now = datetime.datetime.now()
n = now.strftime("%Y-%m-%d")
df2 = None
result = {"時間":[],
            "標題":[],
            "連結":[],
            "內容":[],
            "來源":"中國時報"}
for page in range(1,11): 
  url = "https://covid-19.chinatimes.com/%E6%96%B0%E5%86%A0%E8%82%BA%E7%82%8E,%E5%8F%B0%E7%81%A3?page="+str(page)
  response = requests.get(url, timeout = (2,3))
  soup = BeautifulSoup(response.text, 'html.parser')
  # print(soup)

  tags=soup.select(".col h3")
  date_tags=soup.select(".meta-info .date")
  time_tags=soup.select(".meta-info .hour")
  contents = soup.select(".intro")
  #print(contents)

  for j in contents:
      result["內容"].append(j.text)
      #print(j.text)
      
  for i in tags:
    result["標題"].append(i.text[3:])
      # print(title,link)
    result["連結"].append("https://www.chinatimes.com"+i.find_all('a')[-1]["href"])
  
  for k in time_tags:
    time= k.text
  for l in date_tags:
    date= l.text
    result["時間"].append(date+" "+time)
'''
print(len(result["內容"]  ))
print(len(result["時間"]  ))
print(len(result["標題"]  ))


'''
df = pd.DataFrame(result)
  #print(df)

df['時間'] = pd.to_datetime(df['時間']) 
print(df)
# df = df.set_index('時間') 

# if df2 == None:
#   df2 = df[n]
# else:
#   df2.append(df)


# print(df2)
df.to_csv("./chinatimes_covid_content.csv")

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/Users/user/Downloads/chromedriver')
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import datetime

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

options = Options()
options.add_argument("--disable-notifications")

driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
driver.get("https://tw.appledaily.com/search/%E8%82%BA%E7%82%8E/") 
for x in range(1, 11):
    driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
    content = BeautifulSoup(driver.page_source, "html.parser")

result = {"時間":[],"標題":[],"連結":[],"內容":[],"來源":"蘋果日報"} 
allTitle = content.select( ".flex-feature" )
allTime = content.select(".timestamp")

for t  in allTitle:
  links = "https://tw.appledaily.com/" + t.find_all('a')[-1]["href"]
  #print(links)
  result["連結"].append("https://tw.appledaily.com/" + t.find_all('a')[-1]["href"])
  
  r = requests.get(links)
  web_content = r.text
  soup = BeautifulSoup(web_content,'lxml')

  articleContent = soup.find_all('p')
  articleContent

  article = []
  for p in articleContent:
    article.append(p.text)

  articleAll = '\n'.join(article)
  result["內容"].append(articleAll)


for k in allTime:
    result["時間"].append(k.text[5:])
    #print(k.text[5:])
  
for i in allTitle:
    result["標題"].append(i.text[22:])
    #print(i.text[22:])
'''
print(len(result["內容"]  ))
print(len(result["時間"]  ))
print(len(result["標題"]  ))

''' 
df = pd.DataFrame(result)
#print(df)
now = datetime.datetime.now()
n = now.strftime("%Y-%m-%d")
df['時間'] = pd.to_datetime(df['時間']) 
# df = df.set_index('時間') 
#print(df[n])
df.to_csv("./apple_news.csv")
driver.close()
print()

!pip install selenium
!apt-get update # to update ubuntu to correctly run apt install
!apt install chromium-chromedriver
!cp /usr/lib/chromium-browser/chromedriver /usr/bin
import sys
sys.path.insert(0,'/Users/user/Downloads/chromedriver')
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import time
import datetime

chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

options = Options()
options.add_argument("--disable-notifications")

driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
web = driver.get("https://health.udn.com/health/cate/120949#category")


for x in range(1, 11):
    driver.execute_script("window.scrollTo(0,document.body.scrollHeight)")
    content = BeautifulSoup(driver.page_source, "html.parser")
    #print(content)

result = {"時間":[],
          "標題":[],
          "連結":[],
          "內容":[],
          "來源":"聯合報"}

time= content.select(".dt time")
for j in time:
  #print(j.text)
  result["時間"].append(j.text)

allTitle = content.select("h3")   
for i in allTitle[:50]:
  result["標題"].append(i.text)
  #print(i.text)

link = content.select("dt > a")
for t in link[24:74]:
  links = "https://health.udn.com/" + t.get("href")
  result["連結"].append("https://health.udn.com/" + t.get("href"))
  #print(t.get("href"))
  
  r = requests.get(links)
  web_content = r.text
  soup = BeautifulSoup(web_content,'lxml')

  articleContent = soup.find_all('p')
  articleContent

  article = []
  for p in articleContent:
    article.append(p.text)

  articleAll = '\n'.join(article)
  result["內容"].append(articleAll)



print(len(result["內容"]  ))
print(len(result["時間"]  ))
print(len(result["標題"]  ))


df = pd.DataFrame(result)
print(df)

df.to_csv("./udn_covid_news.csv")

import os
import pandas as pd
import glob
from datetime import datetime, timedelta
import time

csv_list = glob.glob('*.csv') #查看同文件夾下的csv文件數
print(u'共發現%s個CSV文件'% len(csv_list))
print(u'正在處理............')
for i in csv_list: #循環讀取同文件夾下的csv文件
    fr = open(i,'rb').read()
    with open('0705result.csv','ab') as f: #將結果保存爲result.csv
        f.write(fr)
print(u'合併完畢！')